{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [高等資料探勘與巨量資料分析 作業一：文字共現關聯分析 (Keyword Correlation Analysis)](https://github.com/NCHU-NLP-Lab/110_Advanced-Data-Mining-and-Big-Data-Analysis/blob/main/KCM/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download wiki data (50,000 articles json file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "wikifilename='wiki_2021_10_05_50000.json'\n",
    "download_enable = not(os.path.isfile(wikifilename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#untokenize wiki_data wiki_2021_10_05_50000\n",
    "if(download_enable):\n",
    "    !gdown --id 1rQnbaOiqoN40AzHVq_IrRW4ki-rFPRxZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Jieba斷詞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 下載Jieba繁體中文詞庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictfilename='dict.txt.big'\n",
    "download_enable_1 = not(os.path.isfile(dictfilename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download 繁體中文詞庫\n",
    "if(download_enable):\n",
    "    !wget --directory-prefix='C:\\Users\\ken\\pycode' 'https://www.dropbox.com/s/ikv3n0fzb218vgn/dict.txt.big?dl=1'\n",
    "    # 修改繁體中文 dict file name\n",
    "    !mv /content/dict.txt.big?dl=1 /content/dict.txt.big\n",
    "    # 看一下dict內容\n",
    "    # !head /content/dict.txt.big"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import paddle\n",
    "from tqdm import tqdm #顯示進度條\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paddle enabled successfully......\n"
     ]
    }
   ],
   "source": [
    "# jieba configration\n",
    "jieba.set_dictionary('dict.txt.big') # 設定使用繁體中文字典\n",
    "jieba.case_sensitive = True # 可控制對於詞彙中的英文部分是否為case sensitive, 預設False\n",
    "paddle.enable_static()\n",
    "jieba.enable_paddle() #启动paddle模式, jieba 0.4版後支持"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dict_method_1 = False\n",
    "user_dict_method_2 = False\n",
    "user_dict_method_3 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1: 將所有的title做成名詞斷詞庫\n",
    "\n",
    "if(user_dict_method_1):\n",
    "    with open('userdict.txt', 'a+', encoding='utf-8') as f:\n",
    "        for element in tqdm(data):\n",
    "            f.write(element.get('title')+' n 3\\n')\n",
    "    f.close\n",
    "    user_dist = 'userdict.txt'\n",
    "    jieba.load_userdict(user_dist) #導入自訂詞庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2: 將所有的keywords做成名詞斷詞庫\n",
    "keywords = ['臺灣', '美國', '大學', '肺炎','天安門','歌手','中國','蔡英文','立法院','颱風']\n",
    "if(user_dict_method_2):\n",
    "    with open('userdict.txt', 'w', encoding='utf-8') as f:\n",
    "        for word in tqdm(keywords):\n",
    "            f.write(word+' n 3\\n')\n",
    "    f.close\n",
    "    user_dist = 'userdict.txt'\n",
    "    jieba.load_userdict(user_dist) #導入自訂詞庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from c:\\Users\\ken\\Documents\\NCHU_Bigdata\\HW_KCM\\dict.txt.big ...\n",
      "Loading model from cache C:\\Users\\ken\\AppData\\Local\\Temp\\jieba.u898abf08e0df186584d9d0613cb0b4a9.cache\n",
      "Loading model cost 1.523 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# method 3: use download wiki dict\n",
    "if(user_dict_method_3):\n",
    "    user_dist = 'wiki.dict.txt'\n",
    "    jieba.load_userdict(user_dist) #導入自訂詞庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新竹|的|交通大學|在|新竹|的|大學路|上|\n",
      "新竹(n)|的(n)|交通大學(n)|在(p)|新竹(n)|的(n)|大學路(n)|上(f)|\n",
      "--------------------------------\n",
      "我愛北京天安門|\n",
      "我愛北京天安門(n)|\n",
      "--------------------------------\n",
      "台灣|大學|在|美國|與|中國|都|很|有名|,| |其|中有|一堂課|在|介紹|中國|天安門|事變|的|歷史|,| |引發|蔡英文|總統|發表|看法|,| |並且|在|立法院|中|造成|言論|的|颶風|,|就算|在|新冠|肺炎|的|影響|仍|具有|高度|關注度|\n",
      "台灣(ns)|大學(n)|在(p)|美國(n)|與(zg)|中國(n)|都(d)|很(d)|有名(a)|,(x)| (x)|其(r)|中有(n)|一堂課(x)|在(p)|介紹(x)|中國(n)|天安門(n)|事變(x)|的(n)|歷史(n)|,(x)| (x)|引發(x)|蔡英文(n)|總統(n)|發表(x)|看法(v)|,(x)| (x)|並且(x)|在(p)|立法院(n)|中(n)|造成(v)|言論(x)|的(n)|颶風(n)|,(x)|就算(v)|在(p)|新冠(n)|肺炎(n)|的(n)|影響(x)|仍(zg)|具有(v)|高度(n)|關注度(x)|\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# exapmle for 斷詞 by Jieba-tw\n",
    "sentence = ['新竹的交通大學在新竹的大學路上','我愛北京天安門','台灣大學在美國與中國都很有名, 其中有一堂課在介紹中國天安門事變的歷史, 引發蔡英文總統發表看法, 並且在立法院中造成言論的颶風,就算在新冠肺炎的影響仍具有高度關注度']\n",
    "\n",
    "for i in range(0, 3):\n",
    "  words = jieba.cut(sentence[i])\n",
    "  for word in words:\n",
    "    print(word, end='|')\n",
    "  print()\n",
    "  \n",
    "  words1 = pseg.cut(sentence[i]) # 斷詞+詞性\n",
    "  for word, flag in words1:\n",
    "    print(word+'('+flag+')', end='|')\n",
    "  print()\n",
    "  print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data have been tokenized\n",
    "with open('wiki_2021_10_05_50000.json', 'r', encoding='utf-8') as file:\n",
    "\tdata = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "json file load to data\n",
      "50000\n",
      "<class 'list'>\n",
      "------------------------------\n",
      "the element of data[0]\n",
      "3\n",
      "<class 'dict'>\n",
      "------------------------------\n",
      "review dict material\n",
      "1. element of key\n",
      "dict_keys(['id', 'title', 'articles'])\n",
      "2. get value by key 'articles'\n",
      "克拉西瓦亞梅恰河是俄羅斯的河流，位於圖拉州和利佩茨克州內，屬於頓河的右支流，河道全長244公里，流域面積6,000平方公里，在每年11月下旬開始結冰，直至翌年4月上旬，河畔城鎮有葉夫列莫夫。\n",
      "蠶豆嘧啶（，也稱爲2,6-二氨基-4,5-二羥基嘧啶，）是一種在蠶豆（學名：\"Vicia faba\"）和家山黧豆（學名：\"Lathyrus sativus\"）中發現的生物鹼，是蠶豆嘧啶葡糖苷的糖苷配基，通常情況下是以嘧啶酮式（2,6-二氨基-1-氫-5-羥基-4-嘧啶酮）或嘧啶二酮式互變異構體（2,6-二氨基-1,3-二氫-4,5-嘧啶二酮）存在。\n"
     ]
    }
   ],
   "source": [
    "# 確認一下json file內容\n",
    "print('---'*10)\n",
    "print('json file load to data')\n",
    "print(len(data))\n",
    "print(type(data))\n",
    "print('---'*10)\n",
    "print('the element of data[0]')\n",
    "print(len(data[0]))\n",
    "print(type(data[0]))\n",
    "print('---'*10)\n",
    "print('review dict material')\n",
    "print('1. element of key')\n",
    "print(data[0].keys())\n",
    "print('2. get value by key \\'articles\\'')\n",
    "print(data[0].get('articles'))\n",
    "print(data[1].get('articles'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### programming concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 為了確保是繁體中文, 套一個簡體 => 繁體轉換\n",
    "from opencc import OpenCC\n",
    "cc = OpenCC('s2t') # 初始化轉換器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 去除輸入文字中的網址資料\n",
    "def filter_url_tag(atricle): \n",
    "    import re\n",
    "    rule = re.compile(r'[http|https]://[a-zA-Z0-9.?/&=:]*',re.S)\n",
    "    return re.sub(rule, '', atricle)\n",
    "\n",
    "# 斷詞結果去除停用詞 (form web download list) \n",
    "def filter_stopwords(words, stop_words):\n",
    "    if (stop_words):\n",
    "        fil_words = [w for w in words if w not in stop_words]\n",
    "    return fil_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225\n",
      "<class 'list'>\n",
      "['the', 'of', 'is', 'and', 'to', 'in', 'that', 'we', 'for', 'an', 'are', 'by', 'be', 'as', 'on', 'with', 'can', 'if', 'from', 'which']\n",
      "['一', '不', '在', '人', '有', '為', '以', '於', '上', '他', '後', '之', '來', '因', '下', '可', '到', '由', '這', '也']\n",
      "['則', '怎', '曾', '至', '致', '著', '諸', '自', '，', '。', '；', '、', '」', '「', '！', '!', ',', '[', ']', '~']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "filename='stopwords.txt'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    stop_words = [word.strip('\\n') for word in f]\n",
    "print(len(stop_words))\n",
    "print(type(stop_words))\n",
    "print(stop_words[0:20])\n",
    "print(stop_words[50:70])\n",
    "print(stop_words[160:180])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [03:01<00:00, 275.24it/s]\n"
     ]
    }
   ],
   "source": [
    "if not(os.path.isfile('wiki_tokenize.json')):\n",
    "  for n in tqdm(range(0, len(data))):\n",
    "\n",
    "    element = data[n]\n",
    "    article = element.get('articles') #取得文章內容\n",
    "    article = cc.convert(article) # 轉繁體中文\n",
    "    article = filter_url_tag(article)\n",
    "\n",
    "    words = jieba.lcut(article, cut_all=False) #斷詞\n",
    "    # words = filter_stopwords(words, stop_words)\n",
    "    \n",
    "    element['token'] = words\n",
    "    data[n] = element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014年山西官場地震或稱山西官場大地震、山西塌方式腐敗，是指自2014年起，山西省人民政府、中共山西省委所屬的諸多中高級官員相繼因貪腐問題被調查，引起全國關注。僅在8月，中紀委就宣佈對四位副部級高官的調查。此次事件被官方定性爲“系統性、塌方式腐敗”，山西塌方式腐敗與時任全國政協副主席令計劃關係密切，而時任中共山西省委書記袁純清也因此被調離山西。2013年12月26日至27日，中共山西省委十屆五次全會召開時，省委常委共13人：2014年9月1日，中共中央政治局常委劉雲山、中央組織部部長趙樂際赴太原宣佈山西省委主要負責人職務調整：王儒林任山西省委書記，袁純清不再擔任山西省委書記。9月30日，中共山西省委召開常委擴大會議，宣佈中共中央關於山西省黨政班子調整補充的決定。此時，省委常委的組成是：\n",
      "['2014年山西官場地震', '或稱', '山西', '官場', '大地震', '、', '山西', '塌', '方式', '腐敗', '，', '是', '指自', '2014年', '起', '，', '山西省人民政府', '、', '中共', '山西省', '委', '所屬', '的', '諸多', '中高級', '官員', '相繼', '因', '貪腐', '問題', '被', '調查', '，', '引起', '全國', '關注', '。', '僅在', '8月', '，', '中紀委', '就', '宣佈', '對', '四位', '副部級', '高官', '的', '調查', '。', '此次', '事件', '被', '官方', '定性', '爲', '“', '系統性', '、', '塌', '方式', '腐敗', '”', '，', '山西', '塌', '方式', '腐敗', '與', '時任', '全國政協副主席', '令計劃', '關係密切', '，', '而', '時任', '中共山西省委書記', '袁純清', '也', '因此', '被', '調離', '山西', '。', '2013年', '12月26日', '至', '27', '日', '，', '中共', '山西省', '委', '十屆', '五次', '全會', '召開', '時', '，', '省委常委', '共', '13', '人', '：', '2014年', '9月1日', '，', '中共中央政治局常委', '劉雲山', '、', '中央組織部', '部長', '趙樂際', '赴', '太原', '宣佈', '山西省', '委', '主要', '負責人', '職務', '調整', '：', '王儒林', '任', '山西', '省委書記', '，', '袁純清', '不再', '擔任', '山西', '省委書記', '。', '9月30日', '，', '中共', '山西省', '委', '召開', '常委', '擴大', '會議', '，', '宣佈', '中共中央', '關於', '山西省', '黨政', '班子', '調整', '補充', '的', '決定', '。', '此時', '，', '省委常委', '的', '組成', '是', '：']\n"
     ]
    }
   ],
   "source": [
    "print(data[2].get('articles'))\n",
    "print(data[2].get('token'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save done\n"
     ]
    }
   ],
   "source": [
    "# 斷詞結果存為json file\n",
    "if not(os.path.isfile('wiki_tokenize.json')):\n",
    "    with open('wiki_tokenize.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f)\n",
    "    f.close\n",
    "    print('save done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KCM-keyword Correlation Models from Open Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load wiki tokenized data and reference QA/Ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "dict_keys(['id', 'title', 'articles', 'token'])\n",
      "['克拉西瓦亞梅恰河', '是', '俄羅斯', '的', '河流', '，', '位於', '圖拉州', '和', '利佩茨克州', '內', '，', '屬於', '頓河', '的', '右', '支流', '，', '河道', '全長', '244', '公里', '，', '流域面積', '6', ',', '000', '平方公里', '，', '在', '每年', '11月', '下旬', '開始', '結冰', '，', '直至', '翌年', '4月', '上旬', '，', '河畔', '城鎮', '有', '葉夫列莫夫', '。']\n"
     ]
    }
   ],
   "source": [
    "# data have been tokenized\n",
    "# if data already have element, don't run read file step\n",
    "\n",
    "if ('token' not in data[0].keys()):\n",
    "    with open('wiki_tokenize.json', 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    print('loading done')\n",
    "\n",
    "print(len(data))\n",
    "print(data[0].keys())\n",
    "print(data[0].get('token'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quation rule: only calculation 名詞類別 數量\n",
    "flag_list = ['n','ng','nr','nrfg','nrt','ns','nt','nz'] #Part of Speech list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['臺灣', '美國', '大學', '肺炎', '天安門', '歌手', '中國', '蔡英文', '立法院', '颱風']\n"
     ]
    }
   ],
   "source": [
    "# 取得reference QA (keywords) from file\n",
    "with open(\"ref_qa.txt\", 'r', encoding='utf-8') as file:\n",
    "    keywords = file.read().split(' ')\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['日本', '香港', '中國大陸', '分佈', '中國', '中華民國', '日治', '臺北市', '名稱', '臺北'], ['非建制地區', '城市', '人口普查', '加拿大', '英國', '地區', '加利福尼亞州', '國家', '伊利諾伊州', '公司'], ['學院', '學生', '美國', '課程', '研究', '畢業', '學校', '教授', '查理', '教育'], ['病例', '報告', '冠狀病毒', '新冠', '湖南省', '疫情', '感染者', '傳染性', '當日', '感染'], ['支隊', '中隊', '母親', '北京', '中國人民武裝警察部隊北京市總隊', '警衛', '學生', '國旗', '大隊', '丁子霖'], ['歌曲', '專輯', '演員', '音樂', '流行', '香港', '日本', '單曲', '韓國', '節目'], ['日本', '特有植物', '古代', '國家', '分佈', '廣東省', '印度', '美國', '臺灣', '研究'], ['總統', '民進黨', '主席', '中華民國總統', '韓國瑜', '批評', '時任', '宋楚瑜', '總統候選人', '民主進步黨'], ['立法委員', '黨團', '行政院', '條例', '委員會', '國會', '民進黨', '質詢', '法案', '中華民國'], ['聯合颱風警報中心', '升格', '強度', '等級', '颶風', '薩菲', '辛普森', '級別', '莫拉克', '日本氣象廳']]\n"
     ]
    }
   ],
   "source": [
    "# 取得reference answer for keywords\n",
    "with open(\"ref_ans.txt\", 'r', encoding='utf-8') as file:\n",
    "    ref_ans = [word.strip('\\n').split(' ') for word in file]\n",
    "print(ref_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### begin 統計\n",
    "concept:\n",
    "1. 如果keyword在token內, 再進行詞性分析(by jieba.posseg as pseg)\n",
    "2. 詞性分析後, 如果是名詞類, 則進行統計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:45<00:00,  4.57s/it]\n"
     ]
    }
   ],
   "source": [
    "# 依據keywords數量開始刷tokenized data\n",
    "data2 = []\n",
    "for n in tqdm(range(0, len(keywords))):\n",
    "    qa = keywords[n]\n",
    "    temp ={}\n",
    "    \n",
    "    # 開始刷\n",
    "    for m in range(0, len(data)):\n",
    "        token = data[m].get('token')\n",
    "        article = data[m].get('articles')\n",
    "        # print(' '.join(map(str, token))) # 為了增加重作斷詞的正確性, 把list轉為空格隔開的字串 (所以下面要再處理)\n",
    "        \n",
    "        \"\"\"\n",
    "        (bad method, 太久了!!!!)\n",
    "        if (qa in token):\n",
    "            jbresult = pseg.cut(' '.join(map(str, token))) # 重新做一次jieba斷詞, 取得詞性\n",
    "            for (word, flag) in jbresult:\n",
    "                if (word != ' ') and (flag in flag_list): # <= 移除空格\n",
    "                    # print(word, end='|')\n",
    "                    temp[word] =  temp.get(word, 0) + 1\n",
    "        \"\"\"\n",
    "\n",
    "        if qa in article:\n",
    "            for word in token:\n",
    "                if word not in stop_words:\n",
    "                    temp[word] = temp.get(word, 0) + 1\n",
    "\n",
    "    data2.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'dict'>\n",
      "['HMV', '源自', '英國', '跨國', '連鎖', '名稱', '牠', '主人', '聲音', '縮寫', '小狗', '聽', '留聲機', '商標', '原本', '從事', '生產', '發行', '音樂', '唱片', '後來', '不再', '改為', '零售', '首間', '店', '1921年', '倫敦', '開業', '現時']\n",
      "['臺灣', '·', '中國', '以及', '進行', '開始', '香港', '日本', '地區', '使用', '主要', '美國', '包括', '其中', '擔任', '當時', '位於', '成立', '獲得', '例', '同時', '成為', '發展', '之後', '地', '約', '可以', '自己', '電影', '活動']\n",
      "[5941, 2210, 2106, 1998, 1976, 1966, 1928, 1919, 1795, 1711, 1583, 1573, 1516, 1508, 1455, 1383, 1269, 1261, 1259, 1253, 1240, 1214, 1212, 1203, 1202, 1191, 1191, 1163, 1123, 1095]\n"
     ]
    }
   ],
   "source": [
    "print(type(data2))\n",
    "print(type(data2[0]))\n",
    "print(list(data2[0].keys())[0:30])\n",
    "\n",
    "element = data2[0]\n",
    "ans = {k: v for k, v in sorted(element.items(),reverse=True , key=lambda item: item[1])}\n",
    "\n",
    "print(list(ans.keys())[0:30])\n",
    "print(list(ans.values())[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='stopwords.txt'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    stop_words = [word.strip('\\n') for word in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---   臺灣   ---\n",
      "中國 n|香港 n|日本 n|地區 n|美國 n|發展 n|電影 n|活動 n|公司 n|專輯 n|部分 n|\n",
      "------------------------------------------------------------\n",
      "---   美國   ---\n",
      "中國 n|公司 n|電影 n|研究 n|發現 n|國家 n|可能 n|地區 n|日本 n|英國 n|面積 n|\n",
      "------------------------------------------------------------\n",
      "---   大學   ---\n",
      "中國 n|美國 n|研究 n|學生 n|學校 n|畢業 n|香港 n|發展 n|臺灣 n|學院 n|國家 n|\n",
      "------------------------------------------------------------\n",
      "---   肺炎   ---\n",
      "病例 n|病毒 n|報告 n|香港 n|美國 n|冠狀病毒 n|研究 n|出院 n|治癒 n|朝鮮 n|疫情 n|\n",
      "------------------------------------------------------------\n",
      "---   天安門   ---\n",
      "趙紫陽 n|中國 n|周恩來 n|江澤民 n|焦裕祿 n|北京 n|學生 n|鄧小平 n|毛澤東 n|香港 n|領導 n|\n",
      "------------------------------------------------------------\n",
      "---   歌手   ---\n",
      "專輯 n|歌曲 n|單曲 n|音樂 n|電影 n|演唱會 n|演出 n|美國 n|推出 n|香港 n|作品 n|\n",
      "------------------------------------------------------------\n",
      "---   中國   ---\n",
      "香港 n|地區 n|臺灣 n|日本 n|美國 n|發展 n|國家 n|公司 n|研究 n|上海 n|中國大陸 n|\n",
      "------------------------------------------------------------\n",
      "---   蔡英文   ---\n",
      "臺灣 n|總統 n|民進黨 n|中華民國 n|國民黨 n|郝柏村 n|綠黨 n|立法院 n|主席 n|媒體 n|總統府 n|\n",
      "------------------------------------------------------------\n",
      "---   立法院   ---\n",
      "臺灣 n|總統 n|國民黨 n|選舉 n|中華民國 n|民進黨 n|代表 n|中國 n|政府 n|委員會 n|郝柏村 n|\n",
      "------------------------------------------------------------\n",
      "---   颱風   ---\n",
      "聯合颱風警報中心 n|香港 n|日本氣象廳 n|升格 n|熱帶氣旋 n|熱帶低氣壓 n|公里 n|臺灣 n|發展 n|熱帶風暴 n|澳門 n|\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ans = []\n",
    "# Top10 related word\n",
    "for n in range(0, len(data2)):\n",
    "    print('---  ', keywords[n], '  ---')\n",
    "    element = data2[n]\n",
    "\n",
    "    # sorted by values (word frequency)\n",
    "    element = {k: v for k, v in sorted(element.items(),reverse=True , key=lambda item: item[1])}\n",
    "    # review sorted result\n",
    "    # print(list(element.keys())[0:30])\n",
    "    # print(list(element.values())[0:30])\n",
    "\n",
    "    T10_related_words = []\n",
    "\n",
    "    article = ' '.join(map(str,list(element.keys())))\n",
    "    jbresult = pseg.cut(article)\n",
    "\n",
    "    for (word, flag) in jbresult:\n",
    "        if (flag in flag_list) and (word not in stop_words) and (word != keywords[n]) and (len(word)>1):\n",
    "            T10_related_words.append(word)\n",
    "            print(word, flag, end='|')\n",
    "        if (len(T10_related_words)>10):\n",
    "            break\n",
    "    # print(T10_related_words)\n",
    "    print('\\n'+'---'*20)\n",
    "\n",
    "    ans.append(T10_related_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 8171.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "臺灣:  3 / 10\n",
      "美國:  4 / 10\n",
      "大學:  6 / 10\n",
      "肺炎:  4 / 10\n",
      "天安門:  2 / 10\n",
      "歌手:  5 / 10\n",
      "中國:  5 / 10\n",
      "蔡英文:  3 / 10\n",
      "立法院:  3 / 10\n",
      "颱風:  3 / 10\n",
      "---------------------------------------------\n",
      "limit: 3\n",
      "得分: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 以參考答案計算得分\n",
    "score = []\n",
    "score_val = 0\n",
    "limit = 3\n",
    "\n",
    "for n in tqdm(range(0, len(keywords))):\n",
    "    print(keywords[n], end=':  ')\n",
    "\n",
    "    right = 0\n",
    "    for a in ans[n]:\n",
    "        if a in ref_ans[n]:\n",
    "            right += 1\n",
    "    print(right,'/',len(ref_ans[n]))\n",
    "    score.append(right)\n",
    "    if right >= limit:\n",
    "        score_val += 1\n",
    "\n",
    "print('---'*15)\n",
    "print('limit:', limit)\n",
    "print('得分:', score_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summary by ken in 2021, Oct\n",
    "1. KCM計算分數主要受斷詞影響\n",
    "2. 使用Jieba斷詞方便易用, 但user_dict and stopwords是關鍵\n",
    "3. 統計上相關性應該可以用TF-IDF取代, 例如 stopwords 應該就是出現在很多篇文章的詞, 應該重要性要被下修(懲罰)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## redo TF-IDF by SK-learn package\n",
    "### [文字探勘中的 TF-IDF 技術](https://clay-atlas.com/blog/2020/08/01/nlp-%E6%96%87%E5%AD%97%E6%8E%A2%E5%8B%98%E4%B8%AD%E7%9A%84-tf-idf-%E6%8A%80%E8%A1%93/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1535.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# review data by transfer token list to article\n",
    "test_article = []\n",
    "for n in tqdm(range(0, 1000)): #len(data))):\n",
    "    words = [w for w in data[n].get('token') if w not in stop_words]\n",
    "    temp = ' '.join(map(str, words))\n",
    "    test_article.append(temp)\n",
    "# print(test_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (Temp/ipykernel_12044/3156575000.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\ken\\AppData\\Local\\Temp/ipykernel_12044/3156575000.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    vectorizer = CountVectorizer()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "    vectorizer = CountVectorizer()\n",
    "    transformer = TfidfTransformer()\n",
    "    tfidf = transformer.fit_transform(vectorizer.fit_transform(test_article))\n",
    "    words = vectorizer.get_feature_names()  #所有文本的关键字\n",
    "    weight = tfidf.toarray()\n",
    "    \n",
    " "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "508e50d5139014a86d1fdafdd4aff9ed1d53bb03657ae4a6f9b87556d4dc7371"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('PyCode': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
